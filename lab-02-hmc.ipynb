{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Lab 2: Hamiltonian Monte Carlo\n",
    "\n",
    "In this lab, we'll implement Hamiltonian Monte Carlo (HMC) from scratch using JAX and compare it to a basic random walk Metropolis sampler.\n",
    "\n",
    "**What's HMC?**\n",
    "HMC is a Markov Chain Monte Carlo (MCMC) method that uses gradient information to make intelligent proposals. Instead of random walks, HMC simulates Hamiltonian dynamics. This lets HMC:\n",
    "- Make large moves while maintaining higher acceptance rates\n",
    "- Efficiently explore distributions with complex geometry (correlations, curved modes)\n",
    "- Scale much better to higher dimensions than random walk methods\n",
    "\n",
    "**Learning objectives:**\n",
    "- Understand the mechanics of the leapfrog integrator\n",
    "- Implement HMC and gain more familiarity with JAX\n",
    "- Gain intuition for MCMC hyperparameters (step size, trajectory length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from jax import grad, jit\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "plt.rcParams[\"font.size\"] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## Part A: The Target Distribution (~5 min)\n",
    "\n",
    "We'll sample from a challenging distribution: **four banana-shaped modes** arranged symmetrically. This is difficult for random walk samplers because:\n",
    "\n",
    "1. **Mode separation**: The modes are far apart, so a random walk must take many small steps to move between them\n",
    "2. **Curved geometry**: Each mode is banana-shaped, so even within a mode, random walks struggle to follow the curved high-probability region\n",
    "\n",
    "HMC can handle both challenges by using gradient information to make informed proposals that follow the geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prob(theta):\n",
    "    \"\"\"Four banana-shaped modes log-probability.\n",
    "\n",
    "    Args:\n",
    "        theta: Array of shape (2,) containing [x, y] coordinates\n",
    "\n",
    "    Returns:\n",
    "        Log probability (unnormalized)\n",
    "    \"\"\"\n",
    "    x, y = theta[0], theta[1]\n",
    "    a, b = 0.5, 1.0  # Banana curvature params\n",
    "    d = 3.5  # Mode separation\n",
    "\n",
    "    # Four banana modes pointing outward from origin\n",
    "    log_ps = jnp.array(\n",
    "        [\n",
    "            -0.5 * ((x - d) ** 2 + ((y - d) - a * (x - d) ** 2) ** 2 / b),  # top-right\n",
    "            -0.5 * ((x + d) ** 2 + ((y - d) + a * (x + d) ** 2) ** 2 / b),  # top-left\n",
    "            -0.5 * ((x - d) ** 2 + ((y + d) + a * (x - d) ** 2) ** 2 / b),  # bottom-right\n",
    "            -0.5 * ((x + d) ** 2 + ((y + d) - a * (x + d) ** 2) ** 2 / b),  # bottom-left\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return jax.scipy.special.logsumexp(log_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the target distribution\n",
    "x = jnp.linspace(-8, 8, 200)\n",
    "y = jnp.linspace(-8, 8, 200)\n",
    "X, Y = jnp.meshgrid(x, y)\n",
    "positions = jnp.stack([X.ravel(), Y.ravel()], axis=-1)\n",
    "\n",
    "# Evaluate log prob at each point\n",
    "log_probs = jax.vmap(log_prob)(positions).reshape(X.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.contourf(X, Y, jnp.exp(log_probs), levels=50, cmap=\"RdPu\")\n",
    "plt.colorbar(label=\"Probability density\")\n",
    "plt.xlabel(r\"$\\theta_x$\")\n",
    "plt.ylabel(r\"$\\theta_y$\")\n",
    "plt.title(\"Target Distribution: Four Banana Modes\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Part B: Random Walk Metropolis (~5 min)\n",
    "\n",
    "First, let's implement a baseline: the **random walk Metropolis** algorithm. This is the simplest MCMC method:\n",
    "\n",
    "1. **Propose**: From current position $\\theta$, propose $\\theta' \\sim \\mathcal{N}(\\theta, \\sigma^2 I)$\n",
    "2. **Accept/Reject**: Accept with probability $\\alpha = \\min\\left(1, \\frac{p(\\theta' \\mid x)}{p(\\theta \\mid x)}\\right) = \\min\\left(1, \\frac{p(x \\mid \\theta') \\, p(\\theta')}{p(x \\mid \\theta) \\, p(\\theta)}\\right)$\n",
    "\n",
    "The proposal is isotropic — it doesn't know anything about the geometry of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rw_step(key, theta, log_prob_fn, step_size):\n",
    "    \"\"\"Single random walk Metropolis step.\n",
    "\n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        theta: Current position, shape (D,)\n",
    "        log_prob_fn: Function that returns log probability\n",
    "        step_size: Standard deviation of Gaussian proposal\n",
    "\n",
    "    Returns:\n",
    "        new_theta: New position (may be same as old if rejected)\n",
    "        accepted: Boolean indicating if proposal was accepted\n",
    "    \"\"\"\n",
    "    key1, key2 = jr.split(key)\n",
    "\n",
    "    # Propose: add Gaussian noise\n",
    "    theta_prop = theta + step_size * jr.normal(key1, shape=theta.shape)\n",
    "\n",
    "    # Compute log acceptance probability\n",
    "    log_alpha = log_prob_fn(theta_prop) - log_prob_fn(theta)\n",
    "\n",
    "    # Accept or reject\n",
    "    u = jr.uniform(key2)\n",
    "    accepted = jnp.log(u) < log_alpha\n",
    "\n",
    "    new_theta = jnp.where(accepted, theta_prop, theta)\n",
    "    return new_theta, accepted\n",
    "\n",
    "\n",
    "def run_rw_sampler(key, log_prob_fn, theta_init, n_samples, step_size):\n",
    "    \"\"\"Run random walk Metropolis sampler.\n",
    "\n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        log_prob_fn: Function that returns log probability\n",
    "        theta_init: Initial position, shape (D,)\n",
    "        n_samples: Number of samples to generate\n",
    "        step_size: Standard deviation of Gaussian proposal\n",
    "\n",
    "    Returns:\n",
    "        samples: Array of shape (n_samples, D)\n",
    "        acceptance_rate: Fraction of accepted proposals\n",
    "    \"\"\"\n",
    "    keys = jr.split(key, n_samples)\n",
    "\n",
    "    # JIT compile with log_prob_fn captured in closure\n",
    "    @jit\n",
    "    def jit_rw_step(key, theta):\n",
    "        return rw_step(key, theta, log_prob_fn, step_size)\n",
    "\n",
    "    # scan_fn: takes carry (current theta) and input (random key),\n",
    "    # returns new carry (updated theta) and output (theta, accepted) to collect\n",
    "    def scan_fn(theta, key):\n",
    "        new_theta, accepted = jit_rw_step(key, theta)\n",
    "        return new_theta, (new_theta, accepted)\n",
    "\n",
    "    _, (samples, accepted) = jax.lax.scan(scan_fn, theta_init, keys)\n",
    "    acceptance_rate = accepted.mean()\n",
    "\n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the random walk sampler\n",
    "key = jr.PRNGKey(42)\n",
    "theta_init = jnp.array([3.5, 3.5])  # Start near one mode\n",
    "\n",
    "rw_samples, rw_acceptance = run_rw_sampler(key, log_prob, theta_init, n_samples=10000, step_size=1.0)\n",
    "\n",
    "print(f\"Random Walk acceptance rate: {rw_acceptance:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": "---\n## Part C: Implement HMC (~20 min)\n\nNow it's your turn! HMC improves on random walk by simulating Hamiltonian dynamics.\n\n### The Physics Picture\n\nImagine a ball rolling on a surface where height = $-\\log p(\\theta)$. We augment our parameters $\\theta$ with \"momentum\" variables $\\rho$ and define the **Hamiltonian** (total energy):\n\n$$H(\\theta, \\rho) = \\underbrace{-\\log p(\\theta \\mid x)}_{\\text{potential energy } U(\\theta)} + \\underbrace{\\frac{1}{2}\\rho^T\\rho}_{\\text{kinetic energy } K(\\rho)}$$\n\n### The Leapfrog Integrator\n\nTo simulate the dynamics, we use the **leapfrog integrator**. For each step:\n\n$$\\rho_{t+\\epsilon/2} = \\rho_t + \\frac{\\epsilon}{2} \\nabla \\log p(\\theta_t \\mid x) \\quad \\text{(half step for momentum)}$$\n$$\\theta_{t+\\epsilon} = \\theta_t + \\epsilon \\cdot \\rho_{t+\\epsilon/2} \\quad \\text{(full step for position)}$$\n$$\\rho_{t+\\epsilon} = \\rho_{t+\\epsilon/2} + \\frac{\\epsilon}{2} \\nabla \\log p(\\theta_{t+\\epsilon} \\mid x) \\quad \\text{(half step for momentum)}$$\n\n### The HMC Algorithm\n\n1. Sample fresh momentum: $\\rho \\sim \\mathcal{N}(0, I)$\n2. Compute initial Hamiltonian: $H(\\theta, \\rho) = -\\log p(\\theta \\mid x) + \\frac{1}{2}\\rho^T\\rho$\n3. Run leapfrog for $L$ steps with step size $\\epsilon$ to get proposal $(\\theta', \\rho')$\n4. Compute proposed Hamiltonian: $H(\\theta', \\rho')$\n5. Accept with probability $\\alpha = \\min\\left(1, \\exp(-\\Delta H)\\right)$ where $\\Delta H = H(\\theta', \\rho') - H(\\theta, \\rho)$\n\n### Your Task\n\nOpen `hmc.py` and implement the two functions:\n1. `leapfrog(theta, rho, log_prob_fn, epsilon, L)` — the leapfrog integrator\n2. `hmc_step(key, theta, log_prob_fn, epsilon, L)` — the full HMC transition"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses your hmc_step implementation\n",
    "def run_hmc_sampler(key, log_prob_fn, theta_init, n_samples, epsilon, L):\n",
    "    \"\"\"Run HMC sampler.\n",
    "\n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        log_prob_fn: Function that returns log probability\n",
    "        theta_init: Initial position, shape (D,)\n",
    "        n_samples: Number of samples to generate\n",
    "        epsilon: Leapfrog step size\n",
    "        L: Number of leapfrog steps per HMC iteration\n",
    "\n",
    "    Returns:\n",
    "        samples: Array of shape (n_samples, D)\n",
    "        acceptance_rate: Fraction of accepted proposals\n",
    "    \"\"\"\n",
    "    keys = jr.split(key, n_samples)\n",
    "\n",
    "    # JIT compile the step function for speed\n",
    "    @jit\n",
    "    def jit_hmc_step(key, theta):\n",
    "        return hmc_step(key, theta, log_prob_fn, epsilon, L)\n",
    "\n",
    "    # scan_fn: takes carry (current theta) and input (random key),\n",
    "    # returns new carry (updated theta) and output (theta, accepted) to collect\n",
    "    def scan_fn(theta, key):\n",
    "        new_theta, accepted = jit_hmc_step(key, theta)\n",
    "        return new_theta, (new_theta, accepted)\n",
    "\n",
    "    _, (samples, accepted) = jax.lax.scan(scan_fn, theta_init, keys)\n",
    "    acceptance_rate = accepted.mean()\n",
    "\n",
    "    return samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "### Test Your Implementation\n",
    "\n",
    "Run the cells below to test. If correct, you should see:\n",
    "- Leapfrog conserves energy (small $|\\Delta H|$)\n",
    "- HMC produces samples with correct statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "# Test 1: Leapfrog should conserve energy on a simple Gaussian\ndef test_leapfrog_energy():\n    \"\"\"Test that leapfrog approximately conserves energy.\"\"\"\n\n    def simple_log_prob(theta):\n        return -0.5 * jnp.sum(theta**2)\n\n    theta0 = jnp.array([1.0, 0.5])\n    rho0 = jnp.array([0.5, -0.3])\n\n    # Initial energy\n    H0 = -simple_log_prob(theta0) + 0.5 * jnp.sum(rho0**2)\n\n    # Run leapfrog\n    theta_new, rho_new = leapfrog(theta0, rho0, simple_log_prob, epsilon=0.1, L=50)\n\n    # Final energy\n    H1 = -simple_log_prob(theta_new) + 0.5 * jnp.sum(rho_new**2)\n\n    energy_error = jnp.abs(H1 - H0)\n    print(f\"Initial energy: {H0:.6f}\")\n    print(f\"Final energy:   {H1:.6f}\")\n    print(f\"Energy error:   {energy_error:.6f}\")\n\n    if energy_error < 0.01:\n        print(\"\\n✓ Leapfrog test PASSED\")\n    else:\n        print(\"\\n✗ Leapfrog test FAILED - energy not conserved\")\n\n\ntest_leapfrog_energy()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: HMC should produce correct samples on a simple Gaussian\n",
    "def test_hmc_samples():\n",
    "    \"\"\"Test that HMC produces samples with correct mean.\"\"\"\n",
    "    true_mean = jnp.array([1.0, -0.5])\n",
    "\n",
    "    def gaussian_log_prob(theta):\n",
    "        return -0.5 * jnp.sum((theta - true_mean) ** 2)\n",
    "\n",
    "    key = jr.PRNGKey(123)\n",
    "    samples, acc_rate = run_hmc_sampler(\n",
    "        key, gaussian_log_prob, theta_init=jnp.zeros(2), n_samples=2000, epsilon=0.2, L=10\n",
    "    )\n",
    "\n",
    "    # Check mean (discard first 500 as burn-in)\n",
    "    sample_mean = samples[500:].mean(axis=0)\n",
    "    mean_error = jnp.max(jnp.abs(sample_mean - true_mean))\n",
    "\n",
    "    print(f\"True mean:      {true_mean}\")\n",
    "    print(f\"Sample mean:    {sample_mean}\")\n",
    "    print(f\"Max error:      {mean_error:.4f}\")\n",
    "    print(f\"Acceptance:     {acc_rate:.2%}\")\n",
    "\n",
    "    if mean_error < 0.15 and acc_rate > 0.5:\n",
    "        print(\"\\n✓ HMC samples test PASSED\")\n",
    "    else:\n",
    "        print(\"\\n✗ HMC samples test FAILED\")\n",
    "\n",
    "\n",
    "test_hmc_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the four-banana distribution\n",
    "key = jr.PRNGKey(42)\n",
    "theta_init = jnp.array([3.5, 3.5])\n",
    "\n",
    "hmc_samples, hmc_acceptance = run_hmc_sampler(key, log_prob, theta_init, n_samples=5000, epsilon=0.3, L=20)\n",
    "\n",
    "print(f\"HMC acceptance rate: {hmc_acceptance:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "---\n",
    "## Part D: Compare Random Walk vs HMC (~10 min)\n",
    "\n",
    "Now let's compare the two samplers on our challenging four-banana distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run both samplers with the same initial condition\n",
    "key = jr.PRNGKey(0)\n",
    "key1, key2 = jr.split(key)\n",
    "\n",
    "theta_init = jnp.array([3.5, 3.8])  # Start near top-right mode\n",
    "n_samples = 10000\n",
    "\n",
    "# Random Walk\n",
    "rw_samples, rw_acc = run_rw_sampler(key1, log_prob, theta_init, n_samples, step_size=0.5)\n",
    "\n",
    "# HMC\n",
    "hmc_samples, hmc_acc = run_hmc_sampler(key2, log_prob, theta_init, n_samples, epsilon=0.4, L=10)\n",
    "\n",
    "print(f\"Random Walk acceptance rate: {rw_acc:.2%}\")\n",
    "print(f\"HMC acceptance rate: {hmc_acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples from both methods\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Background contours\n",
    "for ax in axes:\n",
    "    ax.contour(X, Y, jnp.exp(log_probs), levels=10, colors=\"gray\", alpha=0.5)\n",
    "    ax.set_xlabel(r\"$\\theta_x$\")\n",
    "    ax.set_ylabel(r\"$\\theta_y$\")\n",
    "    ax.set_xlim(-8, 8)\n",
    "    ax.set_ylim(-8, 8)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "# Random walk samples\n",
    "axes[0].plot(rw_samples[:, 0], rw_samples[:, 1], \"b.\", alpha=0.1, markersize=2, label=\"Samples\")\n",
    "axes[0].plot(rw_samples[:200, 0], rw_samples[:200, 1], \"r-\", alpha=0.5, linewidth=0.5, label=\"First 200 steps\")\n",
    "axes[0].set_title(f\"Random Walk (acc={rw_acc:.1%})\")\n",
    "axes[0].legend(loc=\"upper left\")\n",
    "\n",
    "# HMC samples\n",
    "axes[1].plot(hmc_samples[:, 0], hmc_samples[:, 1], \"b.\", alpha=0.1, markersize=2, label=\"Samples\")\n",
    "axes[1].plot(hmc_samples[:200, 0], hmc_samples[:200, 1], \"r-\", alpha=0.5, linewidth=0.5, label=\"First 200 steps\")\n",
    "axes[1].set_title(f\"HMC (acc={hmc_acc:.1%})\")\n",
    "axes[1].legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# Random Walk traces\n",
    "axes[0, 0].plot(rw_samples[:, 0], \"b-\", alpha=0.7, linewidth=0.5)\n",
    "axes[0, 0].set_ylabel(r\"$\\theta_x$\")\n",
    "axes[0, 0].set_title(r\"Random Walk — $\\theta_x$\")\n",
    "\n",
    "axes[1, 0].plot(rw_samples[:, 1], \"b-\", alpha=0.7, linewidth=0.5)\n",
    "axes[1, 0].set_ylabel(r\"$\\theta_y$\")\n",
    "axes[1, 0].set_xlabel(\"Iteration\")\n",
    "axes[1, 0].set_title(r\"Random Walk — $\\theta_y$\")\n",
    "\n",
    "# HMC traces\n",
    "axes[0, 1].plot(hmc_samples[:, 0], \"b-\", alpha=0.7, linewidth=0.5)\n",
    "axes[0, 1].set_ylabel(r\"$\\theta_x$\")\n",
    "axes[0, 1].set_title(r\"HMC — $\\theta_x$\")\n",
    "\n",
    "axes[1, 1].plot(hmc_samples[:, 1], \"b-\", alpha=0.7, linewidth=0.5)\n",
    "axes[1, 1].set_ylabel(r\"$\\theta_y$\")\n",
    "axes[1, 1].set_xlabel(\"Iteration\")\n",
    "axes[1, 1].set_title(r\"HMC — $\\theta_y$\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "\n",
    "# Convert samples to arviz InferenceData format\n",
    "rw_data = az.convert_to_inference_data({\"theta_x\": rw_samples[None, :, 0], \"theta_y\": rw_samples[None, :, 1]})\n",
    "hmc_data = az.convert_to_inference_data({\"theta_x\": hmc_samples[None, :, 0], \"theta_y\": hmc_samples[None, :, 1]})\n",
    "\n",
    "# Compute ESS (R̂ requires multiple chains, so we skip it here)\n",
    "print(\"Effective Sample Size (ESS):\")\n",
    "print(f\"  Random Walk: θ_x={az.ess(rw_data)['theta_x'].values:.0f}, θ_y={az.ess(rw_data)['theta_y'].values:.0f}\")\n",
    "print(f\"  HMC:         θ_x={az.ess(hmc_data)['theta_x'].values:.0f}, θ_y={az.ess(hmc_data)['theta_y'].values:.0f}\")\n",
    "\n",
    "# ESS improvement\n",
    "rw_min_ess = min(az.ess(rw_data)[\"theta_x\"].values, az.ess(rw_data)[\"theta_y\"].values)\n",
    "hmc_min_ess = min(az.ess(hmc_data)[\"theta_x\"].values, az.ess(hmc_data)[\"theta_y\"].values)\n",
    "print(f\"\\nESS improvement factor: {hmc_min_ess / max(rw_min_ess, 1):.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## Part E: Hyperparameter Exploration (~10 min)\n",
    "\n",
    "Explore how the step size (`epsilon`) affects HMC performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different step sizes\n",
    "epsilons = [0.01, 0.05, 0.1, 0.2, 0.5, 1.0]\n",
    "results = []\n",
    "\n",
    "key = jr.PRNGKey(42)\n",
    "theta_init = jnp.array([3.5, 3.5])\n",
    "\n",
    "for eps in epsilons:\n",
    "    key, subkey = jr.split(key)\n",
    "    try:\n",
    "        samples, acc = run_hmc_sampler(subkey, log_prob, theta_init, n_samples=1000, epsilon=eps, L=20)\n",
    "        # Compute ESS using arviz\n",
    "        data = az.convert_to_inference_data({\"theta_x\": samples[None, :, 0]})\n",
    "        ess = float(az.ess(data)[\"theta_x\"].values)\n",
    "        results.append((eps, float(acc), ess))\n",
    "        print(f\"ε={eps:.2f}: acceptance={acc:.2%}, ESS={ess:.0f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ε={eps:.2f}: Failed ({e})\")\n",
    "        results.append((eps, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bh0jnhtuom9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "epsilons_arr, accs, ess_vals = zip(*results)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "axes[0].semilogx(epsilons_arr, accs, \"bo-\")\n",
    "axes[0].axhline(y=0.65, color=\"r\", linestyle=\"--\", label=\"Target ~65%\")\n",
    "axes[0].set_xlabel(r\"Step size ($\\epsilon$)\")\n",
    "axes[0].set_ylabel(\"Acceptance rate\")\n",
    "axes[0].set_title(\"Acceptance rate vs step size\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1].semilogx(epsilons_arr, ess_vals, \"go-\")\n",
    "axes[1].set_xlabel(r\"Step size ($\\epsilon$)\")\n",
    "axes[1].set_ylabel(\"ESS\")\n",
    "axes[1].set_title(\"Effective sample size vs step size\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "---\n",
    "## Discussion\n",
    "\n",
    "**HMC vs Random Walk**: You should see that HMC achieves both better exploration of the distribution *and* higher acceptance rates compared to random walk Metropolis. This is because HMC uses gradient information to make informed proposals that follow the geometry of the posterior, rather than proposing blindly in random directions.\n",
    "\n",
    "**A warning about diagnostics**: Even though ESS and other diagnostics might look acceptable, the sampler may not have explored all modes! With highly multimodal distributions like our four-banana example, there is no way to tell from ESS alone whether the posterior is faithfully represented. Even $\\hat{R}$ can be misleading if you've gotten unlucky and multiple chains all end up in similar modes. Always visualize your samples when possible, and be skeptical of diagnostics on multimodal posteriors. There is no substitute to looking!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6330fb9e",
   "metadata": {},
   "source": [
    "---\n",
    "## Bonus: Multiple Chains\n",
    "\n",
    "Running multiple chains in parallel lets us compute $\\hat{R}$ (which measures convergence by comparing between-chain and within-chain variance) and get more robust ESS estimates. Can you modify `run_hmc_sampler` to run `n_chains` chains in parallel using `jax.vmap`?\n",
    "\n",
    "**Hint**: You'll need to:\n",
    "1. Generate different random keys for each chain\n",
    "2. Use different (or the same) initial positions for each chain\n",
    "3. `vmap` over the chain dimension\n",
    "4. Return samples with shape `(n_chains, n_samples, D)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "htba0wdcj76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus: Implement multiple chains here\n",
    "def run_hmc_sampler_multi_chain(key, log_prob_fn, theta_init, n_chains, n_samples, epsilon, L):\n",
    "    \"\"\"Run multiple HMC chains in parallel.\n",
    "\n",
    "    Args:\n",
    "        key: JAX random key\n",
    "        log_prob_fn: Function that returns log probability\n",
    "        theta_init: Initial position, shape (D,) - same for all chains\n",
    "        n_chains: Number of parallel chains\n",
    "        n_samples: Number of samples per chain\n",
    "        epsilon: Leapfrog step size\n",
    "        L: Number of leapfrog steps\n",
    "\n",
    "    Returns:\n",
    "        samples: Array of shape (n_chains, n_samples, D)\n",
    "        acceptance_rates: Array of shape (n_chains,)\n",
    "    \"\"\"\n",
    "    # TODO: Implement this!\n",
    "    # Hint: Use jr.split to get n_chains different keys, then vmap run_hmc_sampler\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds595-ai4science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}